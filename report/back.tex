\section{Background}
\label{sec:background}

Automating experiment design is not without precedent.  Kristine Smith started the field of optimal experiment design in 1918 \cite{smith1918standard}.
It is only recently that robots have been employed to conduct scientific exploration autonomously \cite{wagner2001science},\cite{king2004functional}.  We will discuss below is how current robot scientists reliance on global information prevents them from operating in truly unknown environments.  Additionally, we will cover how previous approaches in sequential decision making from statistics do not necessarily reflect the settings that autonomous robots encounter in the real world.


	Who else has done what?

- Evidence that humans make (approximately) rational decisions (we over and under estimate low and high probabilities)

	- Design of experiments has led to (amongst other things) multi-armed bandit models of sequential experiment design.
	- See also maximum entropy sampling
	- See also mutual information sampling.
- Also consider active learning solutions (they all end up being the same anyway)

	- Robotics research has made robots that conduct exploration, but the only ones that make decisions about whether to investigate something or not do one of three things:
	1. match templates.
	2. seek improbable things.
	3. Engage in opportunistic science - they do something if they have the time.  They don't override human mission objectives.

	1 and 2 say nothing about the information content of the material under investigation.
	3 does not have the level of autonomy that we need for truly long-term or remote operations.

	How?

	What have we previously done:
	- D.R. Thompson's work
	- Only looking at satellite imagery.  Good but not sufficient.
	- Trey's work
	- Using POMDPs not scalable to a planet.
	- Mine. Where does my previous work fall short?
- Not bayesian (not a big deal?)
	- Still has the problem on knowing the number of sampling opportunities remaining.

\subsection{The Secretary Problem}

		- Secretary problem
			- They know what the value of their choice is, in many cases.
			- They know how many candidates there are.
			- We have rates of arrival, they just have a queue.
			- While our sampling time cost implies an upper bound on the number of 
			candidates we don't have a lower bound necessarily.

\subsection{Multi-armed bandits}

		- Multi-armed bandit
			- Assumes you can access any arm at any time
			- Many settings don't have a switching cost between arms.
			- Gittins showed that if you have a switching cost and diminishing returns you can solve the problem.
			- We say is we will be randomly assigned an arm and a switching cost.

\subsection{Optimal Foraging}

	- Work in the 1970's about foraging.  About making value judgements.
		- Key point from Charnov's work is that there has to be diminishing returns
		for extracting from a field (specifically towards an asymptote)
		- Different from our setting is that diminishing your reward in one area 
		contributes to diminishing your reward elsewhere. It isn't like picking
		apples off one tree, and finding a new tree with unpicked apples.


	- Piroli 1999 - has come up with a very similar formulation as the one that 
		Mike and I came up with.
		- Something to consider is that we only get to take one sample per patch 
			because of science differentiation requirements (i.e. RP can't have 
			samples closer than 10cm to each other, maybe as far as 1m)
		- We could also incorporate the different cost it takes to identify things 
			in the scene.
		- We also have different times between patches.
		- Should it come to multiple contexts then we have the contextual bandit 
			problem
				- This means that we could keep different distributions of classes per 
					environment.


\subsection{Science Autonomy}

	- Thompson, Asher Bender and Stephane Williams Group
			- making selections based on maps.  
			- Maximum entropy/mutual information smapling.
			- They assume global knowledge, we don't have that.  It is reasonable
			in many settings of interest, can reduce cost.


	- Previous work in robotic exploration is either dependent on global information or it does not make reasoned decisions about the rest of the world and the sampling opportunities that are immediately available to it.
			- opportunistic science only takes advantage of what is immediately availble to the robot and with surplus sampling budget.
			- whoi and mbari follow set patterns (which is fine, the lawnmower is a good and noble thing) but they use an arbitrary threshold to make the decision to sample or not.  They do not take into account the rest of the environment.


	- Dudek and co., using novelty to slow down the driving an collect more data.
		- Ditto their other paper.

\subsubsection{Our Prior Work}
		- Apply optimal foraging, specifically Charnov's marginal value theorem
		model of foraging to the problem of science autonomy* in the following setting:	
			- Limited sampling budget
			- Attempting to learn distributions not acquire quantities of some object.
			- Trying to learn information objectively without an objective. 
			- Can't know the reward until you sample.
			- It's a fusion of bandits and foraging.  The fact that we are focusing on gaining information gives us diminishing returns which lets us use 
			- Don't have access to all the arms at any given time, which is what makes it a foraging problem.
			- Don't have the choice of what objects you encounter.

As seen above real robots may not be able to predict the rewards they will earn from their actions and have to deal with unreliable arrival rates for sampling opportunities.  These are concerns that are not modelled in typical sequential experiment selection algorithms such as the Multi-armed bandit or secretary problems.  This motivates the problem setting used in this paper, and that describe in detail in the following section.
