\section{Method}
\label{sec:method}

The decision making agents in this paper are tested on several randomly
generated transects.  They are repeatedly presented with an object to sample
and have to make the choice to either take a sample or continue travelling
along the transect.  Like the Robbins' secretary problem the agents are not
permitted to backtrack to avail of a previous opportunity.


Figure \ref{fig:transect} gives an example of a simulated transect. As we can
see there are sampling opportunities of different types scattered along the
path that the robot travels along.  

\begin{figure}[htpd!]
	\centering 
	\includegraphics[width=0.7\textwidth]{transect}
	\caption{A cartoon illustrating a transect a robot might be encountering and how samping opportunities of different types may be distributed along it.  In the field the robot would start at one end of the path and follow it to the other end.  There is one path that the rover may follow across the terrain resulting in it encountering different types of sampling opportunities.}
	\label{fig:transect}
\end{figure}


The primary objective of the robot is to learn distributions behind different
classes of objects.  For example they may be the probability distribution
governing the density of sub surface microbial life in different classes of
soil.  Previous work have identified that texture information can successfully
classify different types of soil material \cite{dunlop2006automatic}.  
We imagine that the classes of objects in
this research could correspond to those soil classes.


\subsection{Experiment}

%Detail excised from intro:
%The paper presents a simulated transect -- a path long which the robot is exploring -- where there are a fixed number of classes of objects for the rover to understand.  Two algorithms are compared in the experiment.  One, the uniform algorithm, is based on principles from optimal design of experiments.  The other, called the foraging algorithm, is based on a combination of techniques from optimal foraging theory and sequential experiment selection and attempts to maximize the productivity of the robot.
%The limitations placed on the rovers are the cost in time to collect a sample and the over all time limit on the mission.  We see that for a wide range of sampling costs the foraging algorithm performs at least as good as the uniform algorithm.  For a plausible range of sampling costs the foraging algorithm presents a significant improvement over the uniform algorithm.

The experiment presented in this paper is a modification of the experiments
presented in \cite{furlong2014sequential} and \cite{furlong2014budgeting}.  In
the prior work agents were equipped with limited sampling budgets.  In this
experiment the agents have an unlimited capacity to take samples, but the time
to take the sample is non-zero and there is an overall limit on the duration of
the mission.  The sampling cost and the overall mission time is given in units of arbitrary time.

As with the previous experiments the agents are not permitted to back track in
the hopes of getting a better opportunity.  The primary reason is to constantly
drive the robot to the end of its exploration mission.  Coverage is an
important part of exploration and permitting.  Additionally making decisions
between a current opportunity, a hypothetical future, and any number of
previously seen but unsampled opportunities is considered a more complex
problem and outside the scope of this paper.

In the experiment there are six different classes of objects the agent may
encounter.  They each have their own arrival rate and their appearance along
the transect are generated with a poisson process.  In this paper the arrival
rates of the different sampling opportunities do not change over the course of
the experiment.  While this is almost certainly not the case for long range
traversals like those seen in the Life in the Atacama Desert Project it is a
reasonable approximation for shorter-range traverses.

\begin{table}[htpd!]
	\centering
	\begin{tabular}{l|ccc}
		Class & Mean & Standard Deviation & Arrival Rate\\
							 & (arbitrary units )  & (arbitrary units) & (arbitrary time)\\
 		\hline
		1 & 0 & 1 & 1\\
		2 & 10 & 0.1 & 0.8 \\
		3 & 0 & 5 & 0.9\\
		4 & 2 & 4 & 1.1\\
		5 & -2 & 4 & 0.05\\
		6 & 0 & 0.1 & 1.1\\
		\hline 
		\\
	\end{tabular}
	\caption{The classes the robot is investigating all have values derived from Gaussian random variables with means and standard deviation given.  Different instances of those classes are encountered in accordance to a Poisson process with the rates specified in the above table.  The units of the distribution's mean and variance can be ignored for the purposes of this experiment.  The arrival rate in this experiment is given in units of arbitrary time.  All time quantities -- arrival rate, mission time, and sampling cost -- can be scaled to the order of the mission at hand.}
	\label{tbl:classes}
\end{table}

\subsection{Algorithms}

	% This is only relevant in the larger work.
	% - figure method.3: texture cam, raw image and texture labelled image.

The experiment builds on the prior work in \cite{furlong2014sequential} and \cite{furlong2014budgeting}.  Here we present two algorithms that are being testing on the simulated transect described above.

\subsubsection{Uniform Sampling}

The Uniform Sampling algorithm attempts to distribute the number of samples it
can collect evenly between the different types of objects present on the
transect.  This is chosen because it was a robustly successful algorithm, as
seen in the prior work \cite{furlong2014sequential} and \cite{furlong2014budgeting}.

The Uniform Sampling algorithm does not consider the time remaining in the
transect, nor the time to complete sampling.  In this setting the algorithm
chooses to sample a class either if it does not have the most samples of all
the encountered classes or if is the max and all other classes have been
sampled an equal number of times.

\subsubsection{Foraging}

The foraging algorithm is an attempt to maximize the productivity of the learning agent along the transect.  We attempt to maximize the number of bits learned per unit time.

The reward for sampling a class is an analog for surprise as
defined by the Koch et al \cite{itti2009bayesian}.  Koch looked at the change
in the distribution that resulted in a Bayesian update.  Because this work
uses a non-parametric kernel density estimation we compare
$\log\left(\frac{\hat{p}(x|D\cup \left\{x\right\})}{\hat{p}(x|D)}\right)$.  To
be compatiable with optimal foraging algorithms, specifically the Marginal
Value Theorem of Charnov \cite{charnov1973optimal}, the reward function must
have dimishing returns.  In the case of information update the Bayes Factor
will eventually converge to approximately 1, likewise our estimated empirical
bayes factor.  We take the log of this approximation such that it converges to
zero as more samples are collected.  Figure \ref{fig:reward} demonstrates the diminishing rewards of sampling a distribution using our reward function.


% 	- figure method.2: The reward function as samples are given to it, for two or three different distributions.

\begin{figure}[htpd!]
	\centering
	\includegraphics[width=0.7\textwidth]{images/cumulative-reward.png}
	\caption{The reward function plotted is the cumulative reward for sampling from a Gaussian distribution with mean 0 and variance 1.  The cumulative reward is averaged over five trials of 4000 samples.  As the number of samples from a distribution is increased the amount of information gained is reduced.  The reward at any point in time can be viewed as the reduction in Shannon surprise of an instantiation of the random variable as a result of incorporating that value into the learned distribution.  It is important to note that the returns of this reward function decrease with the time spent sampling a particular random variable.  The diminishing returns are necessary to use the Marginal Value Theorem formulation from Charnov \cite{charnov1973optimal}.}
	\label{fig:reward}
\end{figure}

The unique innovation in this work is comparing the productivity of an action.  Previous work in foraging maintained the variables of interest in the same units, energy consumed or spent in searching for and extracting resouces.  To enable the foraging agent to compare actions we measure productivity as the average suprise experienced from a sample per unit of time spent to acquire that sample.  In keeping with foraging work of \cite{charnov1973optimal} and \cite{pirolli1999information} the agent decides to move on when the productivity of the current sample is less than the expected productivity of exploring the environment.  Namely the decision rule is:

\begin{align*}
	\frac{\mathbb{E}_{T}\left[surprise_{t}\left(k\right)\right]}{t_{cost}} &\geq \frac{\mathbb{E}_{K}\left[\mathbb{E}_{T}\left[surprise_{t}\left(k\right)\right]\right]}{\mathbb{E}_{K}\left[t_{interarrival(k)}\right] + t_{cost}}\\
\end{align*}

Where $\mathbb{E}_{T}\left[\cdot\right]$ is the empirical expected value over the history of samples the agent has taken, $\mathbb{E}_{K}\left[\cdot\right]$ is the empirical expected value over the different classes. $surprise_{t}(k)$ is the surprise due to the $t$-th sampling of a class $k$.  $t_{cost}$ is the sampling cost in time, and $t_{interarrival(k)}$ is the average interarrival time for a class as the rover has encountered them.



% 	- Combining foraging models with bandit literature 
% 		- The previous work on foraging assumed an inherent value to
% 			options that the agent cared about.  Specifically, energy stored up.
% 		- We use a valuation model taken from bandit literature.  
% 		- We use the reward function from Koch's attention models
% 		- We use the decision making process from foraging. 
% 		- We add the concept of multi-arrival rate things, awareness of time limits.
% 	- Previous work had a limit on the number of samples it could take
% 		- We realized that the actual quantity that limits the exploration process
% 			is time.
% 		- By limiting the time and (in this case) relaxing the limit on sample sizes we more accurately deal with productivity.  
% 	- This experiment models a type of prospecting where the number of samples isn't limited but they do take time. 
% 	- To that end we are looking at productivity.
% 
% 	-  This experiment is more akin to contextual bandits.  
% 	- The image represents a context, the NIRVSS 
% 	- Apply texturecam classification of a scene, as the context
% 	- the choice is to sample or continue
% 
% 	- Productivity 


